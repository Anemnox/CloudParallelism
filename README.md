# Cloud Parallelism Project
Cloud Parallelism Project for TCSS 562 Fall 2024

[Link to doc](https://docs.google.com/document/d/19w5DobRjBiq_SC0tPWki5kRT00WHaMT8jWD0k50QCus/edit?usp=sharing)

References

1. [Distributed Training Concepts](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html#distributed-training-basic-concepts)

2. [Getting Started with Distributed Training on Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training-get-started.html)

3. [Workshop on Distributed Training with Sagemaker](https://github.com/aws-samples/sagemaker-distributed-training-workshop)

4. [Pretraining Wav2Vec2 on Librispeech data](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-pretraining)

5. [Finetuning Wav2Vec2 on Sagemaker](https://github.com/aws-samples/amazon-sagemaker-fine-tune-and-deploy-wav2vec2-huggingface/blob/main/sagemaker-fine-tune-wav2vec2.ipynb)

